\section{Methodology}\label{sec:methodology}
\task{Consider the title. The method might be better than the methodology}

This section will cover the methodology used in this project. In addition, it will cover the tools, datasets, and methods used to generate and keep a \gls{void} description updated.

\subsection{GraphDB}\label{sec:graphdb}
This section will cover the choice of the graph database used in this project. It will also cover the datasets used in this project.

There were many popular choices for databases.~\cite{best-graph-databases} mentions many popular graph databases and lists some of the pros and cons of each, such as Neo4j, Stardog, ArangoDB, and GraphDB. At first, Neo4j was considered, as it was often mentioned in the literature we read, but since it is not a triple store, it is not directly able to use the SPARQL query language~\cite{neo4j:-a-reasonable-RDF-graph-database}. Instead, we would have to learn to use Cypher, Neo4j's graph query language~\cite{cypher-query-language}, or translate a SPARQL query to Cypher.

Instead, we chose to use GraphDB, as it uses the SPARQL query language, was free for the purposes we needed, and it was easy to set up and use, making it a good choice for our project.

Upon choosing a database to work with, we used docker to set up a local instance of GraphDB. By doing this, we could easily manage and use our dataset for testing. We used the guide provided by GraphDB~\cite{docker-graphDB} and followed the steps to set up a local instance of GraphDB.

\subsubsection{Dataset}\label{sec:dataset}
It was debated whether to use a pre-existing dataset or create a simple one for testing. Finally, it was decided to create a simple dataset to test different methods of generating a \gls{void} description. This dataset was small, with around 150 Pokemon, their types, and evolutions. This dataset was created in a .ttl file and imported into GraphDB.
After everything worked as intended, we found a larger pre-existing dataset to show a more realistic example of how a \gls{void} description could be generated. This dataset was the "Waterloo SPRAQL Diversity Test Suit (WatDiv) v0.6", containing 10 million triples. The dataset's contents are not crucial for this project, but they should be a good example of generating a void description from a larger dataset.

\subsection{Void Generation}\label{sec:void}
This section will cover our implementation of void generation.

The first step to generating a \gls{void} description was to use a naive query; this would look at the entire dataset and generate a \gls{void} description based on the data. The naive query was a simple query that counted the number of subjects, predicates, objects, and triples. This was done to have a description that could be continuously updated. After this, every time the dataset is updated, a script would read the changes instead of running the naive query and update the \gls{void} description accordingly. By looking at the elements that have been added and whether or not they already exist in the current dataset. This was done to reduce the time it would take to update a \gls{void} description, by specifying the values to update, instead of looking through the entire dataset every time and creating a new \gls{void} description.

\subsubsection{Void Generation Methods}\label{sec:voidmethods}
This section will detail each part of the pipeline used to create a void description.
\task{Add only the most important parts of our implementation and the rest in the appendix.}

% \begin{listing}[htb!]
%     \begin{minted}{python}
%     \end{minted}
%     \caption{}
%     \label{}
% \end{listing}

% Frequency and size of updates to the dataset may be a constraint worth exploringâ€”frequent minor updates vs infrequent significant updates.

% If we analyze the script and deem certain parts overhead, which could be reduced if the algorithm was implemented directly in the database. If someone asks why we did not implement the algorithms in the database implementation, we need the arguments to back up our decisions. This is also true for other decisions.


%Void update will concern:
%size of the database
%triples in the query
%total running time
%cache size
%Amount of operations made
%Time of subqueries generated from insert
%same, but with void generation

\subsection{Data concerns}\label{sec:concerns}
This section will cover the data that the experiments are concerned with regarding updating the void description and also generating the void description. When looking at experiments for the project, we must consider what data we want to concern. To determine this, we need to look at what data is relevant to the project and what we want to compare between update and generation.

\subsubsection{Size of the database}
A metric that will be looked at is the size of the database. The database size could impact how long it takes to update the void description and how long it takes to generate the void description since, with a larger dataset, more data must be read and processed. For this reason, the size of the dataset will be an interesting metric to measure and compare how it affects the dynamically updating the \gls{void} description and generating the void description from scratch.

\subsubsection{Triples in query}
When inserting data into the dataset, updating the \gls{void} description requires processing the query to know what parts of the current \gls{void} description will be affected and need to be updated. Due to this, the size of the query, I.E., The number of triples in the query, will be an interesting metric to measure and compare how it affects the process of dynamically updating the \gls{void} description. However, the number of triples should not impact generating the \gls{void} description from scratch, as the entire dataset will be read and processed. Due to this, it could give some insight into how effective the dynamically updating the \gls{void} description is, compared to generating the \gls{void} description from scratch.


\subsubsection{Cache size}
When measuring the different metrics, we also have to consider what additional elements can impact the results of each measurement. Such a case could be the cache size, as the cache size could impact the time it takes to read and process the dataset. We will examine how the cache size affects the time it takes to update and generate the \gls{void} description. Cache size could be an interesting metric to consider when considering real use cases, as the cache size could be a factor that could be changed to improve the performance. However, it also has a cost of memory usage. Based on the results, a discussion can be made about whether the pros outweigh the cons and to what degree.



\subsubsection{Total running time}
When measuring the different metrics, we must consider how we measure them against each other. For this project, we will mainly look at the different methods' total running time. The total running time will be the time from the start to the end of the process, from when an update to the dataset is made to when the \gls{void} description is updated or generated. Since a script handles the operations, we can measure the time it takes to run the script and get the total running time of the process.
We can assume that the total time it takes for a \gls{void} description to be generated is static when looking at how the amount of triples in a query affects the run time, as the entire dataset will be read processed every time. However, we can assume that the query size will affect the total running time of updating the \gls{void} description. As a result, this should show a curve where the total running time increases as the size of the query increases. Due to this, we can measure when it is more effective to generate the \gls{void} description from scratch instead of updating the \gls{void} description.

\subsubsection{Time of subqueries generated from insert}
Additionally to looking at the total running time, it could prove beneficial to measure the time for the subqueries generated. Since there are many ways of generating the subqueries, understanding how each affects the running time would help generate smarter subqueries that could reduce total running time. This metric would only be used for the update method, as the generation method does not use subqueries.

\subsubsection{Total number of operations}
Time is not the only metric that can be used to compare the different methods. We can also look at the total number of operations when updating the \gls{void} description. The number of operations might prove more challenging to measure. However, it could give insight into how demanding the processes are since time can be based on many factors, such as hardware, database connection, and other machine processes. Therefore, looking at the number of operations made could give a more accurate comparison between the different methods.