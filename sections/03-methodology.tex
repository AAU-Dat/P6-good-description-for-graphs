\section{Methodology}\label{sec:methodology}
\task{Consider the title. The method might be better than the methodology}

This section covers the methodology, tools, and datasets used. In addition, the section also covers the data that the experiments are concerned with, with regards to updating and generating the void descriptions.

\subsection{GraphDB}\label{sec:graphdb}
GraphDB is hosted in a Docker container, this is done to have a local instance of a database, that can quickly and easily be reset, updated and set up on all machines used for the project. By using the guide provided by GraphDB~\cite{docker-graphDB}, and following the steps given.

When deciding what database to use, there were many popular choices aside from GraphDB, as mentioned in~\cite{best-graph-databases}, that lists different popular databases, in addition to the pros and cons of each. Neo4j was one of the few databases that was considered, as is appeard in much of the literature that was read through in the research phase. However, since it is not a triple store, it is not directly able to use the SPARQL query language~\cite{neo4j:-a-reasonable-RDF-graph-database}. Insted Neo4j uses it's own graph query language, Cypher~\cite{cypher-query-language}. Which would have required an understanding of an entirely new language, or translating a SPARQL query to Cypher. It was deemed more appropriate to use a triple store, that uses SPARQL, such as GraphDB. Additionally, GraphDB was free and simple to use for the purposes of this project.

\subsection{Cache}\label{sec:cache}
When running the queries on the dataset, it is important that any data from the previous query is not saved in the cache. For this problem, there are different solutions that can be used. One solution is to clear the cache between every query. Another solution is to set the cache size to a small size, so the cache does not save the data from the previous query. These two solutions are the ones that the group looked at, when deciding how to solve the problem. Instead of clearing the cache between every query, the cache will be small enough to hold information other than the one query running on GraphDB.

The cache is the memory that is used to store the data that is used the most. By default, when GraphDB is run on the Java Virtual Machine, it takes 50\% of the heap size~\cite{cache-strategy}.
Because the size of the default cache was to big, it was found the cache size for this project should be 0. This can be seen in the docker-compose.yml file in the project, with the -Dgraphdb.page.cache.size=0 flag.

GraphDB uses two indexes in the cache to inference and evaluate queries more effetive space wise. The two indexes are predicate-object-subject (POS) and predicate-subject-object (PSO), these are used to store the data in the cache. Both structures convey the same information, but with a different emphasis on the elements involved, as POS emphasis at predicate to object and PSO emphasis predicate to subject.
The two indexes are used for different purposes. They both benefit on what kind of query is run on the dataset. They are good if a query can be rewritten to subject-predicate (SP) or object-predicate (OP). These two indexes comes on occurences when the quering datasets has a lot of predicates. Or When a query asks for predicate, as if there are a relation between the subject and the object~\cite{graphdb-storage}.

\subsection{Dataset}\label{sec:dataset}
Two datasets were used for this project. A small dataset, that was created for testing, and a larger pre-existing dataset. The small dataset was a dataset of Pokemon. The dataset contained 150 Pokemon, their types, and evolutions. This dataset was created in a .ttl file and imported into GraphDB. The dataset was created with the intention of getting a graps of how graph dataset worked, and to test querries, where the results were known. The dataset was also used to test generating and updating a \gls{void} description.

The larger dataset was the "Waterloo SPRAQL Diversity Test Suit (WatDiv) v0.6", containing 10 million triples. The dataset's contents were not crucial for this project, but they would be a good example of generating a void description from a larger dataset. The larger dataset was used to test the different methods of generating a \gls{void} description, when a working prototype was created, from the smaller Pokemon dataset.

The results of generating and updating a \gls{void} description from a larger dataset would be more interesting, as the dataset is larger and more complex than the smaller dataset, and would give a better idea of how it would perform in a real-world scenario. There will be a more in depth description of what results and metrics were a concern, in future sections.

\subsection{Void Generation and Update}\label{sec:void}
There were two methods for creating a \gls{void} description, generate and update.

The first method was to generate a \gls{void} description from scratch, by looking at the entire dataset and generating a \gls{void} description based on the data. This was done by using a naive query, a simple query that counted the number of subjects, predicates, objects, and triples.

The second method was to update the \gls{void} description, by looking at what elements had been added or removed in a given query, and whether or not the elements already existed in the current dataset. Updating would only be concerned with relevant data, and would update the \gls{void} description accordingly. This was done to reduce the time it would take to update a \gls{void} description, by specifying the values to update, instead of looking through the entire dataset every time and generating a new \gls{void} description.

Generating a \gls{void} description was the simplest method, and therefore the one that appeard to be the most widely used method. This project aimed to improve the performance of creating and updating a \gls{void} descriptions. For this reason both methods were implemented, to compare the performance of the two methods, and to see if updating a \gls{void} description was a viable method.
\subsubsection{Void Generation Methods}\label{sec:voidmethods}
\task{Add only the most important parts of our implementation and the rest in the appendix.}


\subsection{Data concerns}\label{sec:concerns}
To have a better understanding of the results, consideration to what data is relevant must be made. Therefore it must be determined clearly and precisely what data is relevant to the project and what data is not, before the experiments are conducted. The data must give a clear insight in the comparison between updating and generating the \gls{void} description.

\subsubsection{Size of the database}
A metric that will be looked at is the size of the database. The database size could impact how long it takes to update the void description and how long it takes to generate the void description since, with a larger dataset, more data must be read and processed. For this reason, the size of the dataset will be an interesting metric to measure and compare how it affects the dynamically updating the \gls{void} description and generating the void description from scratch.

\subsubsection{Triples in query}
When inserting data into the dataset, updating the \gls{void} description requires processing the query to know what parts of the current \gls{void} description will be affected and need to be updated. Due to this, the size of the query, I.E., The number of triples in the query, will be an interesting metric to measure and compare how it affects the process of dynamically updating the \gls{void} description. However, the number of triples should not impact generating the \gls{void} description from scratch, as the entire dataset will be read and processed. Due to this, it could give some insight into how effective the dynamically updating the \gls{void} description is, compared to generating the \gls{void} description from scratch.

\subsubsection{Cache size}
When measuring the different metrics, additional elements can impact the results of each measurement must be considered. Such a case could be the cache size, as the cache size could impact the time it takes to read and process the dataset. Therefore it will be examined how the cache size affects the time it takes to update and generate the \gls{void} description. Cache size could be an interesting metric to consider when considering real use cases, as the cache size could be a factor that could be changed to improve the performance. However, it also has a cost of memory usage. Based on the results, a discussion can be made about whether the pros outweigh the cons and to what degree.

\subsubsection{Total running time}
When looking at the different metrics, it was important to consider how they they would be measured. For this project, total running time was used. The total running time was be the time from the start to the end of the process, from when an update to the dataset was made to when the \gls{void} description aws updated or generated. Since a script handled the operations, it was possible to simply measure the time it took to run the script and get the total running time of the process.
It was assumed that the total time it took for a \gls{void} description to be generated was static when looking at how the amount of triples in a query affects the run time, as the entire dataset would be read and processed every time. However, it was assumed that query size would affect the total running time when updating the \gls{void} description. As a result, this should show a curve where the total running time increases as the size of the query increases. Based on these assumptions, total running time was used to measure when it was more effective to update the \gls{void} description from instead of generating the \gls{void} description from scratch.

\subsubsection{Time of subqueries generated from insert}
Additionally to looking at the total running time, it could prove beneficial to measure the time for the subqueries generated. Since there were many ways of generating the subqueries, understanding how each affects the running time could help generate smarter subqueries that could reduce total running time. This metric would only be used for the update method, as the generation method did not use subqueries.

\subsubsection{Total number of operations}
Time is not the only metric that can be used to compare the different methods. The total number of operations can also be used as a measureing metric. The number of operations could have proved more challenging to measure, as the method to gather the number of operations was not as straight forward as the total running time. However, it could give insight into how demanding the processes were since time could be based on many factors, such as hardware, database connection, and other machine processes. Therefore, looking at the number of operations made could give a more accurate comparison between the different methods.



% The shape of the insert query is also important, not just the size. For example, duplicates in the data may increase the size of the actual query, but because it contains duplicates an update may be to VoID based on our update script would be 'smart' and therefore not be slow.

% so far bruger vi triples bare helt ubrugte triples fra vores dataset som ikke allerede er i databasen
% de vil have en lignende struktur hvilket er et plus
% de passer også til det data vi har da det egentlig er det samme data

\subsection{Insert Triples}\label{sec:insert-triples}
When inserting triples into the dataset, the shape of the insert query is important, and not only the size of the query. For example, in the case of an insert query containing duplicates, the size of the query increases, but the amount of change in the \gls{void} description is the same. Therefore, the update script should be able to handle duplicates and not update the \gls{void} description for each duplicate.


The data was taken from a larger dataset, that was split into ten smaller datasets. The initial creation of the database to work with, was based on the first of the ten, dataset, and all data that was inserted would be pulled from the other 9 datasets. Because all the data was taken from the same dataset, the data would have a similar structure, which made it easy to work with.

