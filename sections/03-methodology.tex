\section{Methodology}\label{sec:methodology}
\task{Consider the title. The method might be better than the methodology}

This section will cover the methodology used in this project, by covering the tools, datasets, and methods used to generate and keep a \gls{void} description updated. The section will also cover the data that the experiments are concerned with regarding updating the void description and also generating the void description.

\subsection{GraphDB}\label{sec:graphdb}
GraphDB is hosted in a Docker container, this is done to have a local instance of a database, that can quickly and easily be reset, updated and set up on all machines used for the project. By using the guide provided by GraphDB~\cite{docker-graphDB}, and following the steps given.

When deciding what database to use, there were many popular choices aside from GraphDB, as mentioned in~\cite{best-graph-databases}, that lists different popular databases, in addition to the pros and cons of each. Neo4j was one of the few databases that was considered, as is appeard in much of the literature that was read through in the research phase. However, since it is not a triple store, it is not directly able to use the SPARQL query language~\cite{neo4j:-a-reasonable-RDF-graph-database}. Insted Neo4j uses it's own graph query language, Cypher~\cite{cypher-query-language}. Which would have required an understanding of an entirely new language, or translating a SPARQL query to Cypher. It was deemed more appropriate to use a triple store, that uses SPARQL, such as GraphDB. Additionally, GraphDB was free and simple to use for the purposes of this project.

\subsection{Dataset}\label{sec:dataset}
Two datasets were used for this project. A small dataset, that was created for testing, and a larger pre-existing dataset. The small dataset was a dataset of Pokemon. The dataset contained 150 Pokemon, their types, and evolutions. This dataset was created in a .ttl file and imported into GraphDB. The dataset was created with the intention of getting a graps of how graph dataset worked, and to test querries, where the results were known. The dataset was also used to test generating and updating a \gls{void} description.

The larger dataset was the "Waterloo SPRAQL Diversity Test Suit (WatDiv) v0.6", containing 10 million triples. The dataset's contents were not crucial for this project, but they would be a good example of generating a void description from a larger dataset. The larger dataset was used to test the different methods of generating a \gls{void} description, when a working prototype was created, from the smaller Pokemon dataset.

The results of generating and updating a \gls{void} description from a larger dataset would be more interesting, as the dataset is larger and more complex than the smaller dataset, and would give a better idea of how it would perform in a real-world scenario. There will be a more in depth description of what results and metrics were a concern, in future sections.

\subsection{Void Generation and Update}\label{sec:void}
There were two methods for creating a \gls{void} description, generate and update.

The first method was to generate a \gls{void} description from scratch, by looking at the entire dataset and generating a \gls{void} description based on the data. This was done by using a naive query, a simple query that counted the number of subjects, predicates, objects, and triples.

The second method was to update the \gls{void} description, by looking at what elements had been added or removed in a given query, and whether or not the elements already existed in the current dataset. Updating would only be concerned with relevant data, and would update the \gls{void} description accordingly. This was done to reduce the time it would take to update a \gls{void} description, by specifying the values to update, instead of looking through the entire dataset every time and generating a new \gls{void} description.

Generating a \gls{void} description was the simplest method, and therefore the one that appeard to be the most widely used method. This project aimed to improve the performance of creating and updating a \gls{void} descriptions. For this reason both methods were implemented, to compare the performance of the two methods, and to see if updating a \gls{void} description was a viable method.
\subsubsection{Void Generation Methods}\label{sec:voidmethods}
\task{Add only the most important parts of our implementation and the rest in the appendix.}


\subsection{Data concerns}\label{sec:concerns}
To have a better understanding of the results, consideration to what data is relevant must be made. Therefore it must be determined clearly and precisely what data is relevant to the project and what data is not, before the experiments are conducted. The data must give a clear insight in the comparison between updating and generating the \gls{void} description.

\subsubsection{Size of the database}
A metric that will be looked at is the size of the database. The database size could impact how long it takes to update the void description and how long it takes to generate the void description since, with a larger dataset, more data must be read and processed. For this reason, the size of the dataset will be an interesting metric to measure and compare how it affects the dynamically updating the \gls{void} description and generating the void description from scratch.

\subsubsection{Triples in query}
When inserting data into the dataset, updating the \gls{void} description requires processing the query to know what parts of the current \gls{void} description will be affected and need to be updated. Due to this, the size of the query, I.E., The number of triples in the query, will be an interesting metric to measure and compare how it affects the process of dynamically updating the \gls{void} description. However, the number of triples should not impact generating the \gls{void} description from scratch, as the entire dataset will be read and processed. Due to this, it could give some insight into how effective the dynamically updating the \gls{void} description is, compared to generating the \gls{void} description from scratch.

\subsubsection{Cache size}
When measuring the different metrics, additional elements can impact the results of each measurement must be considered. Such a case could be the cache size, as the cache size could impact the time it takes to read and process the dataset. Therefore it will be examined how the cache size affects the time it takes to update and generate the \gls{void} description. Cache size could be an interesting metric to consider when considering real use cases, as the cache size could be a factor that could be changed to improve the performance. However, it also has a cost of memory usage. Based on the results, a discussion can be made about whether the pros outweigh the cons and to what degree.

\subsubsection{Total running time}
When looking at the different metrics, it was important to consider how they they would be measured. For this project, total running time was used. The total running time was be the time from the start to the end of the process, from when an update to the dataset was made to when the \gls{void} description aws updated or generated. Since a script handled the operations, it was possible to simply measure the time it took to run the script and get the total running time of the process.
It was assumed that the total time it took for a \gls{void} description to be generated was static when looking at how the amount of triples in a query affects the run time, as the entire dataset would be read and processed every time. However, it was assumed that query size would affect the total running time when updating the \gls{void} description. As a result, this should show a curve where the total running time increases as the size of the query increases. Based on these assumptions, total running time was used to measure when it was more effective to update the \gls{void} description from instead of generating the \gls{void} description from scratch.

\subsubsection{Time of subqueries generated from insert}
Additionally to looking at the total running time, it could prove beneficial to measure the time for the subqueries generated. Since there were many ways of generating the subqueries, understanding how each affects the running time could help generate smarter subqueries that could reduce total running time. This metric would only be used for the update method, as the generation method did not use subqueries.

\subsubsection{Total number of operations}
Time is not the only metric that can be used to compare the different methods. The total number of operations can also be used as a measureing metric. The number of operations could have proved more challenging to measure, as the method to gather the number of operations was not as straight forward as the total running time. However, it could give insight into how demanding the processes were since time could be based on many factors, such as hardware, database connection, and other machine processes. Therefore, looking at the number of operations made could give a more accurate comparison between the different methods.