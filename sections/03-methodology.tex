\section{Methodology}\label{sec:methodology}
This section covers the methodology, tools, and datasets used. In addition, the section also covers the data that the experiments are concerned with, with regards to updating and generating the \gls{void} descriptions.

\subsection{GraphDB}\label{sec:graphdb}
GraphDB is an \gls{rdf} triple store used to store and query \gls{rdf} data. Ontotext develops GraphDB, which is a commercial product, but there is a free version available that is used for this project. GraphDB is a Java application that can be run on a Java Virtual Machine. GraphDB is a triple store that uses \gls{sparql} to query the data stored in the database~\cite{graphdb-product}.

GraphDB is hosted in a Docker container; this is done to have a local instance of a database that can quickly and easily be reset, updated and set up on all machines used for the project using the guide provided by GraphDB and following the steps given. The characteristics of the machine used to generate the results are seen in \autoref{tab:pc-specs}.

When deciding what database to use, there were many popular choices aside from GraphDB, as mentioned in~\cite{best-graph-databases} which lists different popular databases, in addition to the pros and cons of each. Neo4j was one of the few databases considered, as it appeared in much of the literature in the research phase. However, since it is not a triple store, it cannot directly use \gls{sparql}~\cite{neo4j:-a-reasonable-RDF-graph-database}. Instead, Neo4j uses its graph query language, Cypher~\cite{cypher-query-language}, which would have required understanding an entirely new language or translating a \gls{sparql} query to Cypher. Instead, it was deemed more appropriate to use a triple store that uses \gls{sparql}, such as GraphDB. Additionally, GraphDB was accessible and straightforward to use for this project.

\subsection{Cache}\label{sec:cache}
When running the queries on the dataset, any data from the previous query must not be saved in the cache, as this could impact the results of the following queries, and create poor measurements.

For this problem, different solutions can be used. One solution is to clear the cache between every query. Another solution is to set the cache size too small so the cache does not save the data from the previous query. These two solutions are the ones that the group looked at when deciding how to solve the problem. Instead of clearing the cache between every query, the cache will be small enough to hold information other than the one query running on GraphDB because clearing the cache between every query will take much time, and the group wanted to have the fastest solution possible.

The cache is the memory that is used to store the data that is used the most. By default, when GraphDB is run on the Java Virtual Machine, it takes 50\% of the heap size~\cite{cache-strategy}.
Because the default cache size was too big, it was found that the cache size for this project should be 0. This can be seen in \autoref{lst:graphdb-docker-settings} file in the project, with the -Dgraphdb.page.cache.size=0 flag.


\subsection{Dataset}\label{sec:dataset}
The dataset used was the \gls{watdiv}~\cite{WatDiv} v0.6, containing 10 million triples. The dataset's contents were not crucial for this project, but they would be an excellent example of generating a \gls{void} description from a larger dataset.

The results of generating and updating a \gls{void} description from a larger dataset would be more interesting, as the dataset is larger and more complex than the smaller dataset. In addition, the more extensive dataset would give a better idea of how it would perform in a real-world scenario, with two ideas for testing the performance by making both one big update or many more minor updates to the dataset, as these types of changes are used interchangeably in the real world. In future sections, there will be a more in-depth description of what results and metrics were a concern.

The larger dataset is split into ten smaller datasets. The initial creation of the database to work with is made on the first of the ten splits of \gls{watdiv}, and all data that is inserted would be from the other nine splits of the \gls{watdiv} dataset. Because all the data is taken from the same dataset, the data has a similar structure, making it easy to work with.

The database started with 1,000,000 triples, as the experiments were intended to resemble a practical scenario, where a database was already created and in use.
\subsection{\gls{void} Generation and Update}\label{sec:void}
Two methods were used for creating a \gls{void} description in this paper; generate and update.

The first method was to generate a \gls{void} description from scratch, by looking at the entire dataset and generating a \gls{void} description based on the data. This was done by using a naive query, a simple query that counted the number of subjects, predicates, objects, and triples.

An example of what the generate \gls{sparql} query could look like can be seen in \autoref{lst:naive-query}.


\begin{listing}[htb!]
    \begin{minted}{sparql}          
        SELECT
            (COUNT(*) AS ?totalTriples)
            (COUNT(DISTINCT ?subject) AS ?numSubjects)
            (COUNT(DISTINCT ?predicate) AS ?numPredicates)
            (COUNT(DISTINCT ?object) AS ?numObjects)
            WHERE { ?subject ?predicate ?object . }
    \end{minted}
    \caption{SPARQL for naive query}
    \label{lst:naive-query}
\end{listing}

As can be read, the query counts the number of subjects, predicates, objects, and triples in the entire dataset. This query would be run on the dataset, and the results would be used to generate a \gls{void} description.


The second method was to update the \gls{void} description by looking at an insert query to see what updates are being made; this takes a query and finds out what is being inserted; this is then broken into more minor queries for the sake of seeing if it exists in the database, and update the \gls{void} description on what the queries return. This way, there will not be a need to look at the whole database and update the \gls{void} description, as this method finds out what is being updated and can update the \gls{void} description accordingly.

An example of what the update \gls{sparql} query could look like can be simplified to four different subqueries. The first part of the query would look at the subjects, this can be seen in \autoref{lst:subject-update-query}.

\begin{listing}[htb!]
    \begin{minted}{sparql}          
        SELECT ?resource
            (EXISTS { ?resource ?p ?o } AS ?existing) {
            VALUES ?resource { <ExampleOfSubject> } }
    \end{minted}
    \caption{SPARQL query for if subject exists}
    \label{lst:subject-update-query}
\end{listing}

The second part of the query that looks for predicates, can be seen in \autoref{lst:predicate-update-query}.
\begin{listing}[htb!]
    \begin{minted}{sparql}          
        SELECT ?resource 
        (EXISTS { ?s ?resource ?o } AS ?existing) { 
            VALUES ?resource { <ExampleOfPredicate>}
        } 
    \end{minted}
    \caption{SPARQL query for if predicate exists}
    \label{lst:predicate-update-query}
\end{listing}


The third part of the query that looks for objects, can be seen in \autoref{lst:object-update-query}.

\begin{listing}[htb!]
    \begin{minted}{sparql}          
        SELECT ?resource 
        (EXISTS { ?s ?p ?resource } AS ?existing) { 
            VALUES ?resource { <ExampleOfObject> } 
        }
    \end{minted}
    \caption{SPARQL query for if object exists}
    \label{lst:object-update-query}
\end{listing}

The fourth and final part of the query that looks for triples, can be seen in \autoref{lst:triples-update-query}.

\begin{listing}[htb!]
    \begin{minted}{sparql}          
        SELECT ?triple ?existing { 
            VALUES (?s ?p ?o) {  
            (<ExampleOfSubject> <ExampleOfObject> <ExampleOfPredicate> ) 
        }
        BIND (CONCAT(str(?s), str(?p), str(?o)) 
            AS ?triple)
        BIND (EXISTS { ?s ?p ?o } AS ?existing)
        }
    \end{minted}
    \caption{SPARQL query for if a whole triple exist}
    \label{lst:triples-update-query}
\end{listing}

\autoref{lst:usecase-example} shows how it would look in an actual usecase with one triple, by using UNION for each subquery. The query would have to be run for every triple that is being inserted or deleted, and the results would be used to update the \gls{void} description.

\begin{listing}[htb!]
    \begin{minted}{sparql}          
        SELECT * WHERE { 
        {SELECT ?resource (EXISTS { ?resource ?p ?o } AS ?existing) { VALUES ?resource { <http://db.uwaterloo.ca/~galuc/wsdbm/User12633> } } }UNION 
        { SELECT ?resource (EXISTS { ?s ?resource ?o } AS ?existing) { VALUES ?resource { <http://db.uwaterloo.ca/~galuc/wsdbm/follows> } } }UNION 
        { SELECT ?resource (EXISTS { ?s ?p ?resource } AS ?existing) { VALUES ?resource { <http://db.uwaterloo.ca/~galuc/wsdbm/User39065> } } }
        UNION{ SELECT ?triple ?existing { VALUES (?s ?p ?o) {  (<http://db.uwaterloo.ca/~galuc/wsdbm/User12633>   <http://db.uwaterloo.ca/~galuc/wsdbm/follows>   <http://db.uwaterloo.ca/~galuc/wsdbm/User39065> )
 
        } BIND(CONCAT(str(?s), str(?p), str(?o)) AS ?triple) BIND(EXISTS { ?s ?p ?o } AS ?existing) } } } 
    \end{minted}
    \caption{Full SPARQL query for checking the update of two triples}
    \label{lst:usecase-example}
\end{listing}

Generating a \gls{void} description was the most straightforward method and, therefore, the one that appeared to be the most widely used method~\cite{optimize-SPARQL-queries, aether-tool}. However, this project aimed to improve the performance of creating and updating \gls{void} descriptions. For this reason, both methods were implemented to compare the performance of the two methods and to see if updating a \gls{void} description was viable.

\subsection{Update \gls{void} Query}\label{sec:update-void-query}
By only updating relevant parts of the \gls{void} description, it is assumed that keeping a \gls{void} description updated will be improved, as the \gls{void} description will not have to be recreated every time a database is modified.

Based on an insert query, a new query is created. The new query checks what parts of the insert query already exist in the database. This ensures that only the relevant parts of the \gls{void} description will be updated.

\begin{listing}[htb!]
    \begin{minted}{python}
def create_void_select(dict):
    subjects = dict["subjects"]
    predicates = dict["predicates"]
    objects = dict["objects"]
    all_triples = dict["triples_together"]

    all_subjects = " ".join(subjects)
    all_predicates = " ".join(predicates)
    all_objects = " ".join(objects)
    subject_of_query = f"{{SELECT ?resource (EXISTS 
        {{ ?resource ?p ?o }} AS ?existing) 
        {{ VALUES ?resource {{ {all_subjects} }} }} }}"
    predicate_part_query = f"UNION {{ SELECT ?resource (EXISTS 
        {{ ?s ?resource ?o }} AS ?existing)
        {{ VALUES ?resource {{ {all_predicates} }} }} }}"
    object_part_query = f"UNION {{ SELECT ?resource (EXISTS
        {{ ?s ?p ?resource }} AS ?existing) 
        {{ VALUES ?resource {{ {all_objects} }} }} }}"
    triple_part_query = create_triple_part_of_query(
        dict["triples_individually"])
    final_query = f"SELECT * WHERE {{ {subject_of_query + predicate_part_query + object_part_query + triple_part_query} }}"
    return final_query
    \end{minted}
    \caption{Python code that creates the \gls{void} select query}
    \label{lst:create_void_select}
\end{listing}

The function create\_void\_select, see \autoref{lst:create_void_select} takes a dictionary as input; the dictionary is created from a query to be inserted into the database and analyzed to contain all subjects, predicates, objects, and triples. Next, new strings are made from the dictionary, which are select queries that check whether the subjects, predicates, objects, or triples exist. Finally, each string (here, each string either handles subjects, predicates, objects or triples) is combined to create a full select query.

The returned string is a query that creates a table of what parts do or do not exist in the database. From this table, the number of entities that do not exist is counted, and the value for each part in the \gls{void} description is updated accordingly. By using a \gls{sparql} count on each value, it is possible to avoid getting a huge table. This is not done because the table makes it possible to handle situations where an entity is both an object and a subject. It also makes possible extensions like updates to property partitioning, another thing that could be in a void description.

\subsection{Data Considerations}\label{sec:concerns}
To better understand the results later in the project from the experiments, consideration of what data collection is relevant must be considered. Therefore it must be determined clearly and precisely what data is relevant to the project and what data is not before the experiments are conducted. In addition, the data must give a clear insight into the comparison between updating and generating the \gls{void} description.

\subsubsection{Size of the Database}
A metric that will be looked at is the database size; the number of \gls{rdf} triples will measure the size of the database. The database size could impact how long it takes to update the \gls{void} description and how long it takes to generate the \gls{void} description since, with a larger dataset, more data must be read and processed. For this reason, the size of the dataset will be an important factor to control and compare how it affects the dynamically updating the \gls{void} description and generating the \gls{void} description from scratch.

\subsubsection{Triples in Query}

When inserting data, updating the \gls{void} description requires processing the query to know what parts of the current \gls{void} description will be affected and need to be updated. Due to this, the size of the query, that is the number of triples in the query, will be an interesting factor to control and compare how it affects the dynamic updating of the \gls{void} description. However, the number of triples should not impact generating the \gls{void} description from scratch, outside of being added to the total of the entire dataset, as the entire dataset will be read and processed. This could give some insight into how effective dynamically updating the \gls{void} description is, compared to generating the \gls{void} description from scratch.

\subsubsection{Total Running Time}
It was essential to consider how time would be measured. For this project, total running time is used. The total running time is from the start to the end of the process, from when an update to the dataset is made to when the \gls{void} description is updated or generated. This does not include the time to make the first \gls{void} description, as it should be made when the database is created.

The query size, as in the number of triples changed, is assumed to have the most effect on the total running time when updating the \gls{void} description. This is because the query size affects the number of triples that must be processed and read. Based on these assumptions, the total running time is used to measure when it is more effective to search for the updates in the database before implementing them into the database and updating the \gls{void} description rather than making a new \gls{void} description after every update made to a database.

\subsubsection{Total Number of Operations}
The total number of operations can also be used as a comparison.
The number of operations can be more challenging to measure, as this will be found by analysing the code written for the project. The code can be written in many different ways, so the number of operations can vary. Therefore, gathering the number of operations is more complex than the total running time. However, it could give insight into how demanding the processes were since time could be based on many factors, such as hardware, database connection, and other machine processes. Looking at the number of operations made could give a more accurate comparison between the different methods, but because GraphDB does not give access to internals, we cannot measure this.


\subsection{Insert Triples}\label{sec:insert-triples}
When inserting triples into the dataset, it is not only the size of the insert, but the contents of the insert query are also important. For example, in the case of an insert query containing duplicates, the size of the query increases, but the amount of change in the \gls{void} description is the same. Therefore, the update script should be able to handle duplicates and not update the \gls{void} description for each duplicate. Although, when the script was made, no duplicate triples were inserted, as all the data used in an insert query was taken from an unused dataset. Nevertheless, under all circumstances, the contents of the insert query should be considered when looking at the performance of the update script.




