\section{Discussion}\label{sec:discussion}
The results show that the dynamic update or the regeneration method is more beneficial depending on the query size and the database size.

From \autoref{fig:comparison-generation-vs-update}, it can be seen that as the size of a database increases, the time it takes to generate a \gls{void} description increases at a linear rate, whereas updating the \gls{void} description is barely affected by the size of the database. Instead, updating the \gls{void} description is affected by the size of the query it uses.


The results indicate that the query size is the primary determinant for the performance of the dynamic update method, while the size of the database is more relevant for the generation method. Our findings demonstrate that query sizes up to approximately 8000 triples tend to favor the dynamic update method in terms of speed, particularly when the database size exceeds approximately 2 million triples. However, below dbsizes of 2 million the generation method shows superior performance. Thus, the choice between the two methods should be based on the specific query size and the size of the database in order to optimize efficiency \autoref{fig:comparison-generation-vs-update}.

\subsection{Noisy Results}\label{subsec:noisy-results}
Given the presence of considerable noise in the results, evaluating its potential impact on the validity of the acquired data and the overall outcomes of the experiment becomes relevant.

Each experiment was run ten times to ensure the validity of the results. However, it was clear that many factors had a noticeable impact on the results, such as what machine the experiments were run on, whether or not the machine was used for any other purpose and the internet connection. These factors caused noise in the results. Due to this, the results gathered could be more reliable, and the validity of the results could be improved.

Running the experiment additional times would have mitigated the impact of the noise and helped give a clearer picture of the actual outliers. We could also have ensured the experiments were only run on the machine while the machine was not in use, as this is a factor that we suspect had a significant impact on the results.

Another database could have proven beneficial for the results, as GraphDB caused measurement issues. For example, when working with the database, it was impossible to get internal measurements of the times it took to query the database and create a \gls{void} description. Due to this, all measurements were taken externally on local machines, which introduced the previously mentioned issues.

Had a different database been used that provided a method to get internal measurements, the results could have been more accurate, as overhead from using websockets and http could have been avoided. However, this is not entirely a negative, as the current results show how the methods could perform for an end user, as the same issues could impact the end user.

\subsection{Dynamic Dip}\label{subsec:dynamic-dip}
An interesting observation in the dynamic update, see \autoref{fig:update-querysize-good}, was a significant decrease in the time it took to update the \gls{void} description when the database size was right before 10 million triples. The decrease did not seem to correlate with the number of triples inserted. However, by looking through the data that was inserted, it was found that the size of it was noticeably smaller than the previous data.

This could indicate that the shape of the \gls{rdf} data significantly impacts the time it takes to update the \gls{void} description, but it is not entirely clear exactly what parts of the data are the leading cause for this. For example, the number of characters, or something entirely different causes a decrease in the time it takes to update the \gls{void} description. This is an interesting topic to research in the future by looking at how the shape of the \gls{rdf} data impacts the time it takes to update the \gls{void} description.

\subsection{Use case}\label{subsec:use-case}
There are a few different use cases where the dynamic update method could be beneficial. Such a case could be when a user has a database with a large amount of data that, over time, grows, and the owner wishes to keep a \gls{void} description for it. Over time the \gls{void} description will become outdated unless the owner manually monitors the database and updates the \gls{void} report when needed. However, this is a tedious task, and the owner might forget to update the \gls{void} description over time.

Instead, the owner could set up an update method, ensuring that the \gls{void} description is always current. Automatically updating the \gls{void} description would be beneficial for the owner, as it would save them time and effort. But also, it would be helpful for the database users, as they could be guaranteed that the \gls{void} description is up to date.

As mentioned in \autoref{sec:results}, a dynamic update or a regeneration method is more beneficial depending on factors such as the size of the database and the size of the updates. With a small database, the owner may prefer the regeneration method, as it is faster.

%When could this be used
%When is it relevant to use this
%When is it not relevant to use this
%What is the best method to use



% The experiment outcomes revealed that the dynamic update method demonstrated an average increase in execution time of approximately 0.15 milliseconds per inserted triple, while the generation method showed an average increase of approximately 0.00048 milliseconds per triple already present in the database.


%Notes
%Kan det give mening og opdatere frem for altid og generere på ny? - Vi har observeret en ratio

%Kan vi stole på vores resultater/Er vores resultater valide
%Vi har kørt progremmet 10 gange, men vi har opserveret forskellige ting som havde indflydelse - PC kraft og internet stabilitet/hastiged
%Vi kunne have kørt det flere gange for at give et bedre billede af hvad der var outliers
%Vi kunne have kun kørt når pc'en ikke var i brug, for at fjerne støj
%Vi kunne have brugt en anden DB - Vi kunne ikke måle internt hvilket kunne have givet et bedre billede af resultaterne, da der ville ikke være et local host overhead. - Mere relevant data
%Vores målinger kan dog give et godt billede for hvordan det ville være for en reel slutbruger
%Vi opserverede at størrelsen af data (antal karakterer) havde en stor indflydesle på update, men ikke en oberserverbar indflydelse på regeneration
%Dippet er spændene så snak meget om det


%Vi kunne have talt antallet af operationer hvis det var muligt/Vi kunne finde ud af det

%Futureworks
%Vi kan ikke lige nu se præcis hvorfor dykket sker, men kunne være relevant og kigge på


%When deciding what method to use, one should look at the size of the database after the query has been executed, and then compare it to the size of the query and the size of the database before the query was executed. If the size of the database after the query has been executed is larger than the size of the query, the dynamic update method is faster. However, if the size of the database after the query has been executed is smaller than the size of the query, the generation method is faster. By looking at the size of the database after the query has been executed, one can determine what method is faster.

%That way if the database is large, and the query is small, with no change in the database size after the query has been executed, the dynamic method is faster.

%the dynamic method is faster. But if the database is large, and the query is large, the generation method is faster. If a database is small, and the query is small, the dynamic method is faster. But if the database is small, and the query is large, the generation method is faster. All of this is dependent on the size of the database after the query has been executed. If the database size has not changed, the dynamic method would be faster.

%However, if the database size has changed, the generation method would be faster. If the user has a large database, and the query is large, and knows the database size will change, the generation method is faster. But if the user has a large database, and the query is large, and does not know if the database size will change, the dynamic method is faster. If the user has a large database, and the query is small, and knows the database size will change, the dynamic method is faster. But if the user has a large database, and the query is small, and does not know if the database size will change, the dynamic method is faster, as now the query is small.